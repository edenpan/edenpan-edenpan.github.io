<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Eden</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="computer, finance, some recording">
<meta property="og:type" content="website">
<meta property="og:title" content="Eden">
<meta property="og:url" content="http://blog.sevenpan.com/index.html">
<meta property="og:site_name" content="Eden">
<meta property="og:description" content="computer, finance, some recording">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Eden">
<meta name="twitter:description" content="computer, finance, some recording">
  
    <link rel="alternate" href="/atom.xml" title="Eden" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Eden</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://blog.sevenpan.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-RPC,REST,SOAP" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/23/RPC,REST,SOAP/" class="article-date">
  <time datetime="2018-11-23T05:58:11.000Z" itemprop="datePublished">2018-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="RPC，-REST-SOAP"><a href="#RPC，-REST-SOAP" class="headerlink" title="RPC， REST, SOAP"></a>RPC， REST, SOAP</h2><p><a href="https://blog.apisyouwonthate.com/understanding-rpc-rest-and-graphql-2f959aadebe7" target="_blank" rel="noopener">https://blog.apisyouwonthate.com/understanding-rpc-rest-and-graphql-2f959aadebe7</a>  </p>
<h3 id="RPC-Remote-Procedure-Call-RPC"><a href="#RPC-Remote-Procedure-Call-RPC" class="headerlink" title="RPC Remote Procedure Call (RPC)"></a>RPC Remote Procedure Call (RPC)</h3><p>RPC 是最简单最早的一种API交互。它可以执行其他服务器上的代码，并且当用http或者AMQP实现的时候，它就是一个web API.  RPC只是很多的函数但是是在HTTP API 的上下文中， 导致需要将方法放到URL中并且参数放到查询的字符串或者请求中。<br>当被用来执行CRUD操作的时候，RPC只是用来发送和下载数据字段，但是有一个很不好的地方是客户端需要负责很多东西。客户端为了构造它自己的工作流，需要知道哪个方法在什么时候调用。<br>RPC只是一个概念，但是这个概念有很多的要求，这些要求都有对应的实现:</p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/XML-RPC" target="_blank" rel="noopener">XML-RPC</a></li>
<li><a href="https://en.wikipedia.org/wiki/JSON-RPC" target="_blank" rel="noopener">JSON-RPC</a></li>
<li><a href="https://en.wikipedia.org/wiki/SOAP" target="_blank" rel="noopener">Simple Object Access Protocol(SOAP)</a></li>
</ol>
<p>XML-RPC 和JSON-RPC被使用的并不多，而SOAP被使用在大部分的金融和企业系统中，比如Salesforce，其实我之前的公司也是使用的SOAP。<br>XML-RPC在使用的时候由于XML的数据类型无法完全满足，所以会有一些问题。在XML中，很多东西只是字符串而已， JSON对此有一些改进，但是对于区分不同的数据格式比如整形和小数还是有问题。(<b>似乎这里可以看下，是否xml和json有这两个问题</b>)<br>你需要将元数据放到最顶层用于描述字段对应的数据类型。这就是SOAP的一部分基础，使用<a href="https://en.wikipedia.org/wiki/XML_schema" target="_blank" rel="noopener">XML Schema</a>和一个<a href="https://en.wikipedia.org/wiki/Web_Services_Description_Language" target="_blank" rel="noopener">Web Services Description Language(WSDL)</a> 来解释去到哪和包含了什么。<br>插播下什么是XML Schema 和WSDL吧，毕竟以前接触的还比较多，但是一直不太明白它们。  </p>
<pre><code>XML schema 是一个用于描述XML文件类型的东西。通常表示结构的约束和这个文档的内容的类型，是在XML基础之上的一种约束。这些约束通常被表述为语法条件的组合来治理元素的顺序，内容需要满足布尔预测， 数据类型治理元素的内容和状态，以及更多的特殊规则比如唯一性和参照完整性约束。 咳咳，通俗易懂的说，这个文件用于指定一些约束，比如，这个字段是否可以为可选的，还是必须的，如果一个字段有是否也需要另外的字段的存在。也就是可以将一些业务约束或者说是接口约束写入这个文件中，让其进行检查，而不需要在业务代码中进行检查。比较精致的做法是这样，但是在之前的工作中，一般会弃用这个约束，因为该文件的修改设计到外系统的同步，修改比较麻烦，不如完全放松，接口先通，自己进行内部调整。  
WSDL（网络服务描述语言 Web Services Description Language）是一个基于XML的接口定义语言用来描述一个web服务提供的功能。这个缩写也用来表示一个web服务的WSDL描述，提供一个机器可读的描述来告知这个服务可以被调用，它期望的参数是什么，它将返回什么数据结构。所以，WSDL的目的和一个编程语言的类型签名有一些相似。WSDL将服务描述为网络endpoints或者端口的集合。WSDL说明书提供一个XML格式的文档来达到这个目的。抽象定义的端口和信息被从它们的具体使用或者实例中抽离出来，从而允许定义的复用。端口是通过关联一个网络地址的可重复绑定来定义，端口的集合定义了一个服务。这个对于一个端口具体的协议和数据格式规格组成了一个可复用的绑定，而操作和信息会绑定到具体的网络协议和消息格式。通过这样，WSDL描述了这个Web service的公共接口。上面这段描述有些抽象，后续我考虑补充点例子进来。  
</code></pre><p>这些元数据就像是单位。比如是分还是元，很重要咯。<br>一个现代的RPC实现是<a href="https://grpc.io/" target="_blank" rel="noopener">gRPC</a>，可以被简单的考虑为更好的SOAP。它使用了一个数据类型称为<a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="noopener">ProtoBuff</a>,既需要模式也需要数据实例，非常像是SOAP中的WSDL。<br>GRPC专注于使单次交互尽可能的快，它利用了HTTP/2和Protobuff包要小于JSON的特点。但是JSon也可以很容易的使用。  </p>
<h3 id="Representation-State-Transfer-REST"><a href="#Representation-State-Transfer-REST" class="headerlink" title="Representation State Transfer (REST)"></a>Representation State Transfer (REST)</h3><p>REST 是一个网络范式在2000年被Roy Fielding 提出。REST 是关于客户端和服务器端关系的，服务端的数据通过表达数据为简单的格式来达到可用性。这个格式可以是JSON或者XML或者任何东西。<br>这些表达从不同的来源中描述数据，简单称为资源或者资源的集合，它们都是可以通过称为超媒体控件(HATEOAS)的概念使action和关系变得可发现而潜在地可修改。<br>Hypermedia 是REST的基础，本质上是提供接下来可用的操作，这些操作可以是和数据有关，或者一个”Invoice”资源的例子，它可能被链接到一个支付尝试集合，这样这个用户可以尝试支付订单。<br>这些动作只是链接，但是这里的重点是这个客户知道这里有一个订单是可以支付的通过展示一个”支付”链接，并且如果这个链接不存在，那么这个选项也不应该展示给终端用户。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;data&quot;&quot;: &#123;</span><br><span class="line">    &quot;type&quot;: &quot;invoice&quot;,</span><br><span class="line">    &quot;id&quot;: &quot;093b941d&quot;,</span><br><span class="line">    &quot;attributes&quot;: &#123;</span><br><span class="line">      &quot;created_at&quot;: &quot;2017–06–15 12:31:01Z&quot;,</span><br><span class="line">      &quot;sent_at&quot;: &quot;2017–06–15 12:34:29Z&quot;,</span><br><span class="line">      &quot;paid_at&quot;: &quot;2017–06–16 09:05:00Z&quot;,</span><br><span class="line">      &quot;status&quot;: &quot;published&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;links&quot;: &#123;</span><br><span class="line">    &quot;pay&quot;: &quot;https://api.acme.com/invoices/093b941d/payment_attempts&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这就和RPC不太一样了。下面两个列子可以说明：</p>
<pre><code>客户：我可以和Dr.A说话么，他在么？
RPC：不行
客户： 我检查了他的日程，看起来今天他不在。我想和其他的医生聊聊，看起来Dr.B下午三点有空，我可以那时见到她么？
RPC：可以。
</code></pre><p>可以看到，客户承担大部分需要做的事情，它需要知道所有的数据，然后自己决定，然后才能弄清楚下一步应该做什么。而REST告诉你下一步应该做什么：</p>
<pre><code>客户：我可以和Dr.A说话么，他在么？
REST: Dr.A 现在不在，他明天会回来，但是你有以下的选项。如果你不急，可以留下信息我明天告诉他或者我可以给你预定今天其他的医生，你想要知道今天谁有空么？  
客户：是的，请告诉我今天有谁。
REST:医生B和C，这是他们的介绍。
客户:医生B看起来像是我需要的，请帮我做预约。
REST：预约已经成功，这是详细的预约内容。
</code></pre><p>REST提供了所有相关的信息在回应中，然后这个客户可以选择想要的选项。<br>在服务器中集中状态对于系统为不同的客户提供相同的工作流程很有用。不同于分布式处理全部逻辑，检查数据字段，展示Action的列表等等，对于不同的用户展示不同的结果，REST将他们都统一了。<br>对于更多超媒体控制，可以查看：<a href="https://blog.apisyouwonthate.com/representing-state-in-rest-and-graphql-9194b291d127" target="_blank" rel="noopener">Representing State in REST and GraphQL</a>和 <a href="http://apibusters.com/003-why-hypermedia" target="_blank" rel="noopener">API busters podcast- Episode 3:Why Hypermedia</a>.<br>除了超媒体(最有用但是也是最容易忽视的特点)对于一个系统拥有REST API还有其他的要求：</p>
<ol>
<li>REST必须是无状态的：请求之间没有持续的session。</li>
<li>回应应该是可缓存的：这样可以帮助你的API进行扩展，如果客户遵守这些规则。</li>
<li>REST 关注一致性：如果使用HTTP，应该尽可能使用HTTP特性，而不是创造约定。</li>
</ol>
<p>这些约束的目标是使REST架构可以帮助API持续使用。<br>REST同样也不需要使用模式元数据，那些API开发员最讨厌SOAP的东西。很长一段时间没人构建REST APIs通过schema，但是现在这个变的比较普遍由于<a href="http://json-schema.org/" target="_blank" rel="noopener">JSON Schema</a>。<br>JSON Schema是从XML Schema激发出来的，不是完全的功能一致，但是是HTTP API这些年中最重要的事情之一。<br>很不幸的是，REST变成了一个营销词语在2006-2014之间。它变成了程序员渴望的一种质量衡量方式，而不是去理解，然后就到处使用REST，所以很多系统说他们是REST但是只是RPC加上HTTP词语以及漂亮的URLs。你可能并不能获取任何缓存的东西，也许它只是一堆奇怪的对话，并且你并不能发现任何可以进行下一步的链接。<br>在另外一个方面，一个REST API可以被用为RPC风格的，如果客户端的开发忽略这些链接的话。这是不可取的，但是这是可能的。<br>很多人对于REST的困惑在于他们无法理解“所有额外的烦恼”，比如超媒体控制和HTTP缓存。他们没有看到这点，并且很多觉得RPC是全能的。对于他们来说，最重要的是尽可能快的执行远程代码，但是REST(同样是是高性能的)专注于longevity 和减少客户的耦合。<br>REST理论上可以使用任何的传输协议来提供满足约束的功能，但是没有传输协议可以像HTTP一样有功能性。为了使AMQP符合RET，你需要定义超媒体来控制(可能是下一个你可能调用的信息列表)，一个标准的用来声明AMQP信息的缓存性等， 并且创造很多并不存在的工具。<br>REST没有固定的规格所以导致有这些困惑，并且它也没有具体的实现。目前这有两个很受欢迎的规格提供给REST APIs来使用：  </p>
<ol>
<li><a href="http://www.odata.org/" target="_blank" rel="noopener">OData</a></li>
<li><a href="http://jsonapi.org/" target="_blank" rel="noopener">JSON-API</a></li>
</ol>
<p>如果有API宣传它自己使用了这两，它有可能是一个不错的api。你可以使用这两个作为你的客户端或者你使用一个普通的http客户端，并且自己添加一些润滑油。  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/11/23/RPC,REST,SOAP/" data-id="cjotm8kla00072xrrtvk1h3kr" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-配对交易策略RQ" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/07/配对交易策略RQ/" class="article-date">
  <time datetime="2018-11-07T08:21:17.000Z" itemprop="datePublished">2018-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>#配对交易策略 - 初1<br>又名搬砖策略<br><a href="https://uqer.io/v3/community/share/57b6a1b4228e5b79a375925c" target="_blank" rel="noopener">https://uqer.io/v3/community/share/57b6a1b4228e5b79a375925c</a><br>几个基本的问题：</p>
<pre><code>1. 如何找到股票相关性高的股票？
2. 怎么确定股票比较之间的长期稳定关系
3. 怎么依据长期稳定关系来建立策略
</code></pre><h2 id="另外一个pariTrading的介绍"><a href="#另外一个pariTrading的介绍" class="headerlink" title="另外一个pariTrading的介绍"></a>另外一个pariTrading的介绍</h2><p><a href="https://medium.com/auquan/pairs-trading-data-science-7dbedafcfe5a" target="_blank" rel="noopener">https://medium.com/auquan/pairs-trading-data-science-7dbedafcfe5a</a><br>多重比较偏差：会有增加的错误生成一个明显的p-value的可能性当有太多的测试在运行的时候。 如果100个测试运行在随机数据上，我们应该期望看到5个p-value低于0.05.如果你比较n个证券，你将会执行n((n-1)/2次比较，你将会希望看到很多不正确的p-valus,他们将会随着你增加比较次数而增加。为了避免这个问题，你应该选择小数量的对来测试可能协整并且测试单独测试每一个。<br><b>？？？ 为啥测的多了可能会产生过多的错误的结果？还是不太懂。然后比较小的数量，这个数量应该怎么确定了？</b><br>接下来文章中介绍的是使用标普500的股票数据来产生交易对。文章提到：<br><b>This method is prone to multiple comparison bias and in practice the securities should be subject to a second verification step.</b><br>这里面的二次验证是指？  </p>
<p>Z Score(Value) = (Value - Mean)/Standard Deviation<br>在实际操作中，常常试图对数据做一定的扩展，但是这是在假设了一个底部的分布时候。 通常是正太分布。然而，很多金融数据并不是正太分布的，我们必须很小心，而不是简单的假设为正太分布或者特定的分布，当生成统计时。真正的比率分布可能会十分的肥尾，倾向于极端的价值弄乱了我们的模型，并且产生很大的损失。  </p>
<p><b>fat-tailed distribution</b><br>    <a href="https://en.wikipedia.org/wiki/Fat-tailed_distribution" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Fat-tailed_distribution</a></p>
<pre><code>A fat-tailed distribution is a probability distribution that exhibits a large skewness or kurtosis, relative to that of either a normal distribution or an exponential distribution.  
长尾分布是一个概率分布展示了很大的偏差或者峰度相对于一个正太分布或者一个预期内的分布而说。  
</code></pre><p>产生一个交易信号有以下的几个步骤：  </p>
<ol>
<li>收集可靠的数据，清理数据</li>
<li>从数据中创造特征来识别一个交易信号或者逻辑</li>
<li>特征可以是Moving Average 或者价格数据的比例，相关性或者更多复杂的信号 — 将它们联合起来创造更多的特征。  </li>
<li>通过这些特征来生成信号，比如交易品应该买，买或者中性。  </li>
</ol>
<p>在这篇文章中，产生一个配对交易策略的步骤可以如下：</p>
<ol>
<li>我们试图创造一个信号来告诉我们是否这个比例应买或者卖 在下一个时间里。比如这个ratio是Y<br> Y(t) = Sign(Ratio(t+1) - Ratio(t))</li>
<li>获取数据，此处广告省略了。</li>
<li>分割数据，70%用于训练，30%用于测试。</li>
<li>特征构建<br> 相关特征应该是哪些？我们想要预测ratio的移动方向。我们已经看到两个证券是协整的，所以这个ratio将会在mean值之间上下运动。当前值和mean的差将会用于生成交易信号。<br> 我们使用以下的特征：  <ol>
<li>60天移动平均线：滚动平均值的测量</li>
<li>5天移动平均线： 测量的当前均值</li>
<li>60天的标准偏差</li>
<li>z score： (5d MA — 60d MA) /60d SD</li>
</ol>
</li>
<li><p>模型的选择<br> 研究z-score的图表可以看出来，无论这个值太大或者太小它都将会回复到均值的水平。我们使用+1/-1作为阀值来代表过高或者太低，然后我们可以采用以下模型来生成一个交易信号：<br> Ratio 是buy(1)当z-score低于-1.0时，因为我们期望z score将会回到0，也就是增加.<br> Ratio 是sell(-1)当z-score高于1.0时，因为我们期望z score将会回到0，也就是降低.</p>
</li>
<li><p>训练，校验和优化  </p>
<h3 id="避免过度拟合"><a href="#避免过度拟合" class="headerlink" title="避免过度拟合"></a>避免过度拟合</h3><p>在上述模型中，使用了rolling参数来创建而且也许希望来优化窗口大小。我们也许可以使用一个简单的迭代来计算全部的可能的窗口大小，然后使用最好的表现的窗口大小。但是就是会有过度拟合的问题。为了解决这个问题，这个文章里提到的是使用经济学的解释或者使用算法的特性来选择窗口长度。当然还有一个方法： Kalman filter，这个据说不需要制定一个特定的长度，我准备接下来看下这个。  </p>
</li>
</ol>
<h3 id="更多信息"><a href="#更多信息" class="headerlink" title="更多信息"></a>更多信息</h3><p>上述只是一个简单的开发pairtrading策略的过程，在现实中，我们应该使用更复杂的统计学方式，比如：</p>
<pre><code>1. Hurst Exponent
2. Half-life of mean reversion inferred from an Ornstein-Uhlenbeck process
3. Kalman filters
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/11/07/配对交易策略RQ/" data-id="cjotm8kli000e2xrrbh98ujno" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Somethings About Pyalgotrade" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/17/Somethings About Pyalgotrade/" class="article-date">
  <time datetime="2018-10-17T05:27:39.000Z" itemprop="datePublished">2018-10-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/17/Somethings About Pyalgotrade/">Somethings About Pyalgotrade  - the Optimizing</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://gbeced.github.io/pyalgotrade/docs/v0.20/html/tutorial.html#optimizing" target="_blank" rel="noopener">http://gbeced.github.io/pyalgotrade/docs/v0.20/html/tutorial.html#optimizing</a></p>
<h2 id="The-optimizing"><a href="#The-optimizing" class="headerlink" title="The optimizing"></a>The optimizing</h2><p>优化，💡很简单：</p>
<pre><code>一台server用于：
    1. 提供bars用来运行策略
    2. 提供参数用来运行策略
    3. 记录每一个worker的运行策略的结果
很多worker用来：
    使用server提供的参数和数据来运行策略
</code></pre><p>为了展示这个，我们将会使用一个称为RSI2的策略，该策略需要以下的参数：</p>
<pre><code>一个SMA时间段用于区分趋势。我们将会将这个参数称为entrySMA，并且范围选取在150和250之间。  
一个小一些的SMA时间段用于退出参数。我们称之为exitSMA，范围在5到15之间。  
一个RSI时间段用于进入买或者卖，我们称之为rsiPeriod，范围在2到10之间。  
一个RSI超卖阀值用于进入买入点位，我们称之为overSoldThreshold，并且范围在5和25之间。  
一个RSI超买阀值用于进入卖出点位，我们称之为overBoughtThreshold，范围将在75和95之间。
</code></pre><p><b>所以这里一共有：101 <em> 11 </em> 9 <em> 21 </em> 21 = 4409559</b><br>如果一个策略需要运行0.16秒，那么一共需要8.5天才能算出所有的策略。(4409559 <em> 0.16/3600/24 ～ 8.16 Days)<br>如果是利用上了10台八核的机器，时间将会缩短到2.5个小时。（4409559 </em> 0.16/3600/24/8/10 * 24(h) ~ 2.44 h) 所以我们需要并发。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">from pyalgotrade import strategy</span><br><span class="line">from pyalgotrade.technical import ma</span><br><span class="line">from pyalgotrade.technical import rsi</span><br><span class="line">from pyalgotrade.technical import cross</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RSI2(strategy.BacktestingStrategy):</span><br><span class="line">    def __init__(self, feed, instrument, entrySMA, exitSMA, rsiPeriod, overBoughtThreshold, overSoldThreshold):</span><br><span class="line">        super(RSI2, self).__init__(feed)</span><br><span class="line">        self.__instrument = instrument</span><br><span class="line">        # We&apos;ll use adjusted close values, if available, instead of regular close values.</span><br><span class="line">        if feed.barsHaveAdjClose():</span><br><span class="line">            self.setUseAdjustedValues(True)</span><br><span class="line">        self.__priceDS = feed[instrument].getPriceDataSeries()</span><br><span class="line">        self.__entrySMA = ma.SMA(self.__priceDS, entrySMA)</span><br><span class="line">        self.__exitSMA = ma.SMA(self.__priceDS, exitSMA)</span><br><span class="line">        self.__rsi = rsi.RSI(self.__priceDS, rsiPeriod)</span><br><span class="line">        self.__overBoughtThreshold = overBoughtThreshold</span><br><span class="line">        self.__overSoldThreshold = overSoldThreshold</span><br><span class="line">        self.__longPos = None</span><br><span class="line">        self.__shortPos = None</span><br><span class="line"></span><br><span class="line">    def getEntrySMA(self):</span><br><span class="line">        return self.__entrySMA</span><br><span class="line"></span><br><span class="line">    def getExitSMA(self):</span><br><span class="line">        return self.__exitSMA</span><br><span class="line"></span><br><span class="line">    def getRSI(self):</span><br><span class="line">        return self.__rsi</span><br><span class="line"></span><br><span class="line">    def onEnterCanceled(self, position):</span><br><span class="line">        if self.__longPos == position:</span><br><span class="line">            self.__longPos = None</span><br><span class="line">        elif self.__shortPos == position:</span><br><span class="line">            self.__shortPos = None</span><br><span class="line">        else:</span><br><span class="line">            assert(False)</span><br><span class="line"></span><br><span class="line">    def onExitOk(self, position):</span><br><span class="line">        if self.__longPos == position:</span><br><span class="line">            self.__longPos = None</span><br><span class="line">        elif self.__shortPos == position:</span><br><span class="line">            self.__shortPos = None</span><br><span class="line">        else:</span><br><span class="line">            assert(False)</span><br><span class="line"></span><br><span class="line">    def onExitCanceled(self, position):</span><br><span class="line">        # If the exit was canceled, re-submit it.</span><br><span class="line">        position.exitMarket()</span><br><span class="line"></span><br><span class="line">    def onBars(self, bars):</span><br><span class="line">        # Wait for enough bars to be available to calculate SMA and RSI.</span><br><span class="line">        if self.__exitSMA[-1] is None or self.__entrySMA[-1] is None or self.__rsi[-1] is None:</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        bar = bars[self.__instrument]</span><br><span class="line">        if self.__longPos is not None:</span><br><span class="line">            if self.exitLongSignal():</span><br><span class="line">                self.__longPos.exitMarket()</span><br><span class="line">        elif self.__shortPos is not None:</span><br><span class="line">            if self.exitShortSignal():</span><br><span class="line">                self.__shortPos.exitMarket()</span><br><span class="line">        else:</span><br><span class="line">            if self.enterLongSignal(bar):</span><br><span class="line">                shares = int(self.getBroker().getCash() * 0.9 / bars[self.__instrument].getPrice())</span><br><span class="line">                self.__longPos = self.enterLong(self.__instrument, shares, True)</span><br><span class="line">            elif self.enterShortSignal(bar):</span><br><span class="line">                shares = int(self.getBroker().getCash() * 0.9 / bars[self.__instrument].getPrice())</span><br><span class="line">                self.__shortPos = self.enterShort(self.__instrument, shares, True)</span><br><span class="line"></span><br><span class="line">    def enterLongSignal(self, bar):</span><br><span class="line">        return bar.getPrice() &gt; self.__entrySMA[-1] and self.__rsi[-1] &lt;= self.__overSoldThreshold</span><br><span class="line"></span><br><span class="line">    def exitLongSignal(self):</span><br><span class="line">        return cross.cross_above(self.__priceDS, self.__exitSMA) and not self.__longPos.exitActive()</span><br><span class="line"></span><br><span class="line">    def enterShortSignal(self, bar):</span><br><span class="line">        return bar.getPrice() &lt; self.__entrySMA[-1] and self.__rsi[-1] &gt;= self.__overBoughtThreshold</span><br><span class="line"></span><br><span class="line">    def exitShortSignal(self):</span><br><span class="line">        return cross.cross_below(self.__priceDS, self.__exitSMA) and not self.__shortPos.exitActive()</span><br></pre></td></tr></table></figure>
<p>有以下的函数被定义：</p>
<pre><code>#获取进入SMA的值，range从150到250
getEntrySMA(self)
#获取退出SMA的值，range从5到16
getExitSMA(self)
#获取rsi 时段的值，range从2到10
getRSI(self)
#取消建仓逻辑 --- 这里不太明白为什么要增加
onEnterCanceled(self, position)
#平仓逻辑 --- 这里不太明白为什么要增加    
onExitOk(self, position)
#平仓取消逻辑 --- 这里不太明白为什么要增加    
onExitCanceled(self, position)
#策略执行逻辑
onBars(self, bars)
#做多信号
enterLongSignal(self, bar)
#退出做多信号
exitLongSignal(self)
#做空信号
enterShortSignal(self, bar)
#退出做空信号
exitShortSignal(self)
</code></pre><p>其中，onBars(self, bars)函数用于执行策略逻辑，在onBars中，如果目前有仓位，调用了exitLongSignal() 和 exitShortSignal 用于判断是非要执行平仓操作。如果目前空仓，则调用了enterLongSignal()和enterShortSignal()来决定是否建仓。其中，getBroker().getCash()可以获取当前的现金，0.9应该是估算了交易费用。而其中的enterShort()和enterLong()函数应该是BacktestingStrategy类所含有的函数。  </p>
<p>从上面一段函数基本逻辑可以分析出，Pyalgotrade 有两个基本的类，我需要关注：</p>
<pre><code>from pyalgotrade import strategy
strategy.BacktestingStrategy
position
pyalgotrade.technical
</code></pre><p>然后还有旁枝但是不用太过关心的类：</p>
<pre><code>feed
from pyalgotrade.barfeed import quandlfeed
feed = quandlfeed.Feed()
</code></pre><p>回到优化这个主题上来，server的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import itertools</span><br><span class="line">from pyalgotrade.optimizer import server</span><br><span class="line">from pyalgotrade.barfeed import quandlfeed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def parameters_generator():</span><br><span class="line">    instrument = [&quot;ibm&quot;]</span><br><span class="line">    entrySMA = range(150, 251)</span><br><span class="line">    exitSMA = range(5, 16)</span><br><span class="line">    rsiPeriod = range(2, 11)</span><br><span class="line">    overBoughtThreshold = range(75, 96)</span><br><span class="line">    overSoldThreshold = range(5, 26)</span><br><span class="line">    return itertools.product(instrument, entrySMA, exitSMA, rsiPeriod, overBoughtThreshold, overSoldThreshold)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The if __name__ == &apos;__main__&apos; part is necessary if running on Windows.</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    # Load the bar feed from the CSV files.</span><br><span class="line">    feed = quandlfeed.Feed()</span><br><span class="line">    feed.addBarsFromCSV(&quot;ibm&quot;, &quot;WIKI-IBM-2009-quandl.csv&quot;)</span><br><span class="line">    feed.addBarsFromCSV(&quot;ibm&quot;, &quot;WIKI-IBM-2010-quandl.csv&quot;)</span><br><span class="line">    feed.addBarsFromCSV(&quot;ibm&quot;, &quot;WIKI-IBM-2011-quandl.csv&quot;)</span><br><span class="line"></span><br><span class="line">    # Run the server.</span><br><span class="line">    server.serve(feed, parameters_generator(), &quot;localhost&quot;, 5000)</span><br></pre></td></tr></table></figure>
<p>这个代码处理了三个事情：</p>
<ol>
<li>声明了一个生成器函数根据范围产生不同的参数。</li>
<li>加载csv文件到feed中。</li>
<li>运行server监听5000端口来获取到来的链接。</li>
</ol>
<p>worker 脚本使用了 <code>pyalgotrade.optimizer.worker</code> 模块来并行运行策略，并且使用了server提供的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from pyalgotrade.optimizer import worker</span><br><span class="line">import rsi2</span><br><span class="line"></span><br><span class="line"># The if __name__ == &apos;__main__&apos; part is necessary if running on Windows.</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    worker.run(rsi2.RSI2, &quot;localhost&quot;, 5000, workerName=&quot;localworker&quot;)</span><br></pre></td></tr></table></figure>
<p><b>注意你应该在一台server和大于等于一个worker的情况下运行</b><br>如果你只是想要在你的台式机上并发运行策略，那么可以利用<code>pyalgotrade.optimizer.local</code>模块来处理如下：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import itertools</span><br><span class="line">from pyalgotrade.optimizer import local</span><br><span class="line">from pyalgotrade.barfeed import quandlfeed</span><br><span class="line">import rsi2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def parameters_generator():</span><br><span class="line">    instrument = [&quot;ibm&quot;]</span><br><span class="line">    entrySMA = range(150, 251)</span><br><span class="line">    exitSMA = range(5, 16)</span><br><span class="line">    rsiPeriod = range(2, 11)</span><br><span class="line">    overBoughtThreshold = range(75, 96)</span><br><span class="line">    overSoldThreshold = range(5, 26)</span><br><span class="line">    return itertools.product(instrument, entrySMA, exitSMA, rsiPeriod, overBoughtThreshold, overSoldThreshold)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The if __name__ == &apos;__main__&apos; part is necessary if running on Windows.</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    # Load the bar feed from the CSV files.</span><br><span class="line">    feed = quandlfeed.Feed()</span><br><span class="line">    feed.addBarsFromCSV(&quot;ibm&quot;, &quot;WIKI-IBM-2009-quandl.csv&quot;)</span><br><span class="line">    feed.addBarsFromCSV(&quot;ibm&quot;, &quot;WIKI-IBM-2010-quandl.csv&quot;)</span><br><span class="line">    feed.addBarsFromCSV(&quot;ibm&quot;, &quot;WIKI-IBM-2011-quandl.csv&quot;)</span><br><span class="line"></span><br><span class="line">    local.run(rsi2.RSI2, feed, parameters_generator())</span><br></pre></td></tr></table></figure>
<p>这段代码做了3件事情：</p>
<ol>
<li>声明了一个生成函数用来产出不同的参数组合</li>
<li>从csv中加载到feed中</li>
<li>使用<code>pyalgotrade.optimizer.local</code>来并发运行策略，并且找到最好的组合。  </li>
</ol>
<p>上面一段并发的演示说明如果我想要并发运行，也许我需要看看：  </p>
<pre><code>#单机本地并发代码
from pyalgotrade.optimizer import local
local.run(rsi2.RSI2, feed, parameters_generator())
#多台机器并发的代码
from pyalgotrade.optimizer import worker
worker.run(rsi2.RSI2, &quot;localhost&quot;, 5000, workerName=&quot;localworker&quot;)
</code></pre><p><b>以及说明pyalgotrade的优化部分应该是利用了并发来加速。</b><br>对于我自己的处理，可以考虑加入PSO 优化用于处理。</p>
<p>##</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/10/17/Somethings About Pyalgotrade/" data-id="cjotm8kle00092xrr9b5qcpyd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algotrading-pyalgotrade/">Algotrading, pyalgotrade</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hexoWithGit" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/13/hexoWithGit/" class="article-date">
  <time datetime="2018-10-13T14:18:38.000Z" itemprop="datePublished">2018-10-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/13/hexoWithGit/">Hexo With Git</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="server"><a href="#server" class="headerlink" title="server"></a>server</h2><p>环境：  </p>
<pre><code>ubuntu 16.01LTS
</code></pre><p><b>nginx:</b><br>安装 nginx：</p>
<pre><code>sudo apt-get update
sudo apt-get install nginx
</code></pre><p>配置：  </p>
<pre><code>/etc/nginx/nginx.conf
....
server {
          listen       80 default_server;
          listen       [::]:80 default_server;
          server_name  blog.example.com; # 填写个人域名
          root         /home/where/your/blog/hexo;
  }    
  .....
</code></pre><p>/etc/nginx/sites-enabled 中有一个default文件，如果不需要可以删除，否则启动nginx的时候会报端口冲突。<br>启动nginx：</p>
<pre><code>sudo nginx -t
sudo systemctl reload nginx.service
</code></pre><p><b>git:</b><br>    本部分一大段来自：<a href="https://www.jianshu.com/p/e03e363713f9" target="_blank" rel="noopener">https://www.jianshu.com/p/e03e363713f9</a><br>    安装 git：</p>
<pre><code>sudo apt-get install git-core
mkdir -p /home/git/blog/hexo.git #Git仓库，不存储网站文件
mkdir /home/git/blog/hexo #实际存储网站文件目录
</code></pre><p>初始化空的Git仓库：</p>
<pre><code>git init --bare /home/git/blog/hexo.git
</code></pre><p>进入该仓库，配置post-update hooks（有的可能是post-receive）：</p>
<pre><code>cd /home/git/blog/hexo.git/hooks
sudo nano /home/git/blog/hexo.git/hooks/post-update.sample        
git --work-tree=/home/git/blog/hexo --git-dir=/home/git/blog/hexo.git checkout -f
chmod +x post-update
</code></pre><h2 id="local"><a href="#local" class="headerlink" title="local"></a>local</h2><p>本机设置<br>安装hexo相关：</p>
<pre><code>npm install hexo-cli hexo-server hexo-deployer-git -g
</code></pre><p>安装完成后，初始化：</p>
<pre><code>hexo init ~/myBlog
</code></pre><p>修改hexo的配置可以参考：<a href="https://hexo.io/docs/configuration" target="_blank" rel="noopener">https://hexo.io/docs/configuration</a><br>其中 deploy部分需要修改如下：</p>
<pre><code>deploy:     //发布对应的github账号
type: git
repo: user@remoteServer:/home/git/blog/hexo  //用户名@域名或 IP 地址:/home/git/blog/hexo
branch: master
</code></pre><p>然后还有一点，需要ssh-copy-key 将自己的公钥拷贝到远程机器上，这样可以无需密码访问，具体请自行搜索下。</p>
<p>完成以上之后，部署三条命令：</p>
<pre><code>hexo clean
hexo generate
hexo deploy
</code></pre><h2 id="域名转换"><a href="#域名转换" class="headerlink" title="域名转换"></a>域名转换</h2><p>如果有自己的域名，可以在自己的域名服务商上进行设置，这样就可以直接访问域名了。<br>比如我这个是在name    上，进入<a href="https://www.name.com/zh-cn/account/domain/details/sevenpan.com#dns" target="_blank" rel="noopener">DNS</a>设置自己的ip就可以进行转发了。</p>
<h2 id="Reference-Link"><a href="#Reference-Link" class="headerlink" title="Reference Link"></a>Reference Link</h2><p>Hexo: <a href="https://hexo.io/docs/" target="_blank" rel="noopener">https://hexo.io/docs/</a><br>nginx install :    <a href="https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-16-04" target="_blank" rel="noopener">https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-16-04</a><br><a href="https://websiteforstudents.com/migrate-nginx-root-directory-on-ubuntu-17-04-17-10/" target="_blank" rel="noopener">https://websiteforstudents.com/migrate-nginx-root-directory-on-ubuntu-17-04-17-10/</a><br>name.com 域名解析：<a href="http://www.webgou.info/content/netusage/453/" target="_blank" rel="noopener">http://www.webgou.info/content/netusage/453/</a><br>主要参考的两个网页：<br><a href="https://segmentfault.com/a/1190000010680022" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010680022</a><br><a href="https://www.jianshu.com/p/e03e363713f9" target="_blank" rel="noopener">https://www.jianshu.com/p/e03e363713f9</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/10/13/hexoWithGit/" data-id="cjotm8kl800052xrrspyv9flo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hexo-git-nginx/">hexo,git,nginx</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Shuffle_翻译自spark_programming_guideline" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/23/Shuffle_翻译自spark_programming_guideline/" class="article-date">
  <time datetime="2018-04-23T10:17:04.000Z" itemprop="datePublished">2018-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/23/Shuffle_翻译自spark_programming_guideline/">Spark Programming--- Shuffle Operations 翻译</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Spark-Programming—-Shuffle-operations"><a href="#Spark-Programming—-Shuffle-operations" class="headerlink" title="Spark Programming— Shuffle operations"></a>Spark Programming— Shuffle operations</h1><p>original: <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations</a></p>
<p>一些spark的特定操作将会触发被称为shuffle的事件。Shuffle是Spark用于重新分布数据的机制，这样可以在不同的分区来分组。这通常涉及到在executor和机器之间进行拷贝数据，所以shuffle是一个很复杂并且消耗高的操作。  </p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了了解shuffle期间发生了什么，我们可以考虑reduceByKey操作作为例子。reduceByKey操作生成了一个新的RDD通过所有的单个键值组合为一个元组-关键字和针对与该关键字相关的所有值执行reduce函数的结果。这里的挑战不是所有的值对一个单独的键都在同一个分区上或者甚至说在一台机器上，而是它们必须被重新分布来计算结果。<br>在Spark，数据通常不会跨分区分布到特定操作的必要位置。在计算中，一个单独的任务将会在一个单独的分区上操作-然而为了组织所有的数据来被一个的单独reduceByKey 的reduce任务来执行，Spark需要来执行一个all-to-all操作。它必须读取所有分区来找到所有键的值，然后将它们带到一起跨分区来为每一个键计算最终的结果—这个被称为shuffle。<br>尽管在每一个分区中的新的shuffled数据的元素集是很重要的，同样分区自己的顺序也很重要，而元素之间的顺序就不是了。如果一个想要预测shuffle中的顺序数据那么可以使用：  </p>
<ol>
<li>mapPartitions 来排序每一个分区，比如，.sorted</li>
<li>repartitionAndSortWithinPartitions 来有效分区同时同步重新分区。  </li>
<li>sortBy 创造一个全局的排序的RDD</li>
</ol>
<p>可以引起一个shuffle 的操作包括：repartition 和 coalesce,ByKey的操作，除了counting之外的比如：groupByKey 和reduceByKey,以及join操作比如cogroup 和 join。</p>
<h2 id="性能影响"><a href="#性能影响" class="headerlink" title="性能影响"></a>性能影响</h2><p>Shuffle是一个昂贵的操作因为它涉及到磁盘I/O，数据序列化和网络I/O。为了给shuffle组织数据，spark生成一系列任务-maps用于组织数据，以及一系列reduce任务来聚集它。这个命名系统来自于MapReduce而且并不直接和SparK的map，reduce操作有关。<br>在内部，单独的map任务的结果会被保存在内存中直到它们不适用。然后这些结果会被根据目标分区排序并且写向单一的文件。在reduce方面，任务读取相关的排序块。<br>一定的shuffle操作会消耗明显的数量的堆内存因为它们使用的是在内存中的数据结构来组织记录在传输之前或者之后。明显的，reduceByKey和AggregateByKey创造了这些结构在map阶段，以及 ‘Bykey的操作生成了它们在reduce阶段。当数据不能放进内存中时，Spark将会将这些表散落到硬盘中，会引起而外的磁盘I/O和增加垃圾回收次数。<br>Shuffle同样会生成大量的中间文件在磁盘中。从Spark1.3开始，这些文件被保存直到对应的RDDs不再被使用以及已经被垃圾回收了。这样做是为了shuffle文件不需要被重新创造如果lineage被重新计算时。垃圾回收也许会发生只有在一段很长时间，如果这个应用保留了对RDD的引用或者如果GC没有频繁的发生。这意味着长期运行的spark任务也许会消耗大量的磁盘空间。这个零时的磁盘目录会被spark.local.dir参数所指定。<br>Shuffle行为可以被调整通过一系列的参数。可以参考<a href="http://spark.apache.org/docs/latest/configuration.html#shuffle-behavior" target="_blank" rel="noopener"> Spark Configuration Guide.</a>‘Shuffle Behavior’章节。</p>
<h2 id="Shuffle-Behavior"><a href="#Shuffle-Behavior" class="headerlink" title="Shuffle Behavior"></a>Shuffle Behavior</h2><table>
<thead>
<tr>
<th>属性名称Property Name</th>
<th>默认值Default</th>
<th>含义Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.reducer.maxSizeInFlight</td>
<td>48m</td>
<td>从每一个reduce任务中同步获取的map输出的最大值。由于每一个输出需要我们创造一个缓存来接受它，这个代表了每个任务的固定的内存开销，所以尽量保证它较小除非你有很多内存。</td>
<td></td>
</tr>
<tr>
<td>spark.reducer.maxReqsInFlight</td>
<td>Int.MaxValue</td>
<td>这个配置限制了任意给定点远程请求获取块数。当集群中的主机数量增加的时候，它也许会导致一个非常大数量的内部连接到一到多个节点，引起worker在负载下失败。通过允许它来限制获取请求的数量，这个情况也许会缓解</td>
<td></td>
</tr>
<tr>
<td>spark.reducer.maxBlocksInFlightPerAddress</td>
<td>Int.MaxValue</td>
<td>这个配置限制了每一个从给定端口里的的reduce任务可以获取的远程端口数量。当一个大量的block被一个给定的地址在一次单独获取或者同步获取所请求时，可能会冲垮服务的executor或者Node Manager。这个配置对于减少Node Manager的负载尤为有用当外部的shuffle是被允许的。你可以通过设定一个较低值来减轻这个情况。</td>
<td></td>
</tr>
<tr>
<td>spark.maxRemoteBlockSizeFetchToMem</td>
<td>Long.MaxValue</td>
<td>远程的块将会被获取到磁盘中，当这个块的大小超过了这个配置的值在byte单位上。这个用于避免一个巨大的请求占据了太多的内存。我们可以将这个配置为一个指定的值（比如，200M）。注意到这个配置将会影响到shuffle的获取以及远程块获取的块管理。对于允许了外部shuffle服务的用户，这个特性只会在外部shuffle服务版本高于Spark2。2时有效。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.compress</td>
<td>true</td>
<td>是否压缩map的输出文件，通常是一个好想法。压缩将会使用spark.io.compression.codec.</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.file.buffer</td>
<td>32k</td>
<td>对每一个shuffle文件输出流的在内存中的缓存大小，单位是KiB除非有其他的特别指定。这些缓存减少了硬盘查找和系统调用创建中间shuffle文件的过程。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.io.maxRetries</td>
<td>3</td>
<td>(Netty only)最大自动重复尝试的次数如果这个值没有被设置为0.这个重试逻辑有助于稳定大型的shuffle在长时间的GC暂停或者暂时的网络连接问题上。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.io.numConnectionsPerPeer</td>
<td>1</td>
<td>(Netty only) 节点之间的连接的重复使用为了减少大型集群中重复建立连接的情况。对于有很多硬盘和很少主机的集群，这个将会导致并发行不足以饱和所有硬盘，因此用户可能会考虑增加这个值。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.io.preferDirectBufs</td>
<td>true</td>
<td>(Netty only) 堆外缓冲区在shuffle和缓存块转移期间被用于减少垃圾回收。对于对外缓存内存数量有限的环境，用户也许想要关掉这个来强迫所有的来自于Netty的分配都是在堆上。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.io.retryWait</td>
<td>5s</td>
<td>(Netty only) 在每一次重试直接需要等待多久。最大的延迟时间默认是15秒，maxRetries * retryWait.</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.service.enabled</td>
<td>false</td>
<td>允许外部shuffle服务。这个服务保存了通过executor所写的shuffle文件，这样这个executor可以安全的被移除。这个配置必须被允许如果spark.dynamicAllocation.enabled是“true”。这个外部的shuffle服务必须被启动。查看<a href="http://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">dynamic allocation configuration and setup documentation </a>来获得更多信息。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.service.port</td>
<td>7337</td>
<td>外部shuffle服务将会运行的端口。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.service.index.cache.size</td>
<td>100m</td>
<td>缓存条目限制在指定的内存占用空间中，以字节为单位</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.maxChunksBeingTransferred</td>
<td>Long.MAX_VALUE</td>
<td>在shuffle服务中同一时间最大允许传输的块数量。注意到新来的连接将会被关闭如果达到了最大数量。这个客户端将会尝试重新连接根据shuffle的重试配置（see spark.shuffle.io.maxRetries and spark.shuffle.io.retryWait），如果这个限制也被达到了，那么这个任务将会失败。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.sort.bypassMergeThreshold</td>
<td>200</td>
<td>(Advanced)在基于排序的shuffle管理中，避免合并排序数据如果这里没有map-side的聚合和这里最多有配置的这么多的reduce分区。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.spill.compress</td>
<td>true</td>
<td>是否压缩溢出的数据在shuffle期间</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.accurateBlockThreshold</td>
<td>100 <em> 1024 </em> 1024</td>
<td>阀值是以bytes为单位，高于此值将准确记录HighlyCompressedMapStatus中的shuffle块的大小。这个用于帮助阻止OOM通过避免错误估计了shuffle块大小当获取了shuffle块时。</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.registration.timeout</td>
<td>5000</td>
<td>注册外部shuffle服务的超时时间，单位是毫秒</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.registration.maxAttempts</td>
<td>3</td>
<td>当注册外部shuffle服务失败的时候，我们会重复尝试的最大次数</td>
<td></td>
</tr>
<tr>
<td>spark.io.encryption.enabled</td>
<td>false</td>
<td>允许IO编码。目前支持所有的模式除了Mesos。当使用这个特性的时候，我们推荐RPC编码。</td>
<td></td>
</tr>
<tr>
<td>spark.io.encryption.keySizeBits</td>
<td>128</td>
<td>IO编码的值大小单位为bit。支持的值有128，192和256.</td>
<td></td>
</tr>
<tr>
<td>spark.io.encryption.keygen.algorithm</td>
<td>HmacSHA1</td>
<td>当生成一个IO编码键值时使用的算法。被支持的算法在Java Cryptography Architecture Standard Algorithm Name 文档的KeyGenerator章节中被描述。</td>
<td></td>
</tr>
</tbody>
</table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/04/23/Shuffle_翻译自spark_programming_guideline/" data-id="cjotm8klg000a2xrrs25620mv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Distributed Training- Gradients Sharing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/20/Distributed Training- Gradients Sharing/" class="article-date">
  <time datetime="2018-04-20T06:30:54.000Z" itemprop="datePublished">2018-04-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/20/Distributed Training- Gradients Sharing/">Distributed Training_Gradients Sharing 翻译</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Distributed-Training-Gradients-Sharing"><a href="#Distributed-Training-Gradients-Sharing" class="headerlink" title="Distributed Training: Gradients Sharing"></a>Distributed Training: Gradients Sharing</h1><p>from:<a href="https://deeplearning4j.org/distributed" target="_blank" rel="noopener">https://deeplearning4j.org/distributed</a></p>
<p>从0.9.1开始，Deeplearning4j支持在Apache Spark环境中的分布式训练，以及Aeron作为spark之外的高性能内部节点通讯。<br>这个想法很简单：独立的worker在他们的数据集上计算梯度。<br>在梯度被使用到网络权重上的时候，它们在一个中间存储机制上被聚集（一个机器一个）。<br>在聚集之后，在一些配置的阀值基础上更新值并在网络上传播作为一个稀疏二进制数组。<br>在阀值一下的值被存储并且添加已备未来更新，所以它们没有丢失，但是很少在通讯中延迟。<br>这个阀值的方法减少了网络通讯需求相比于之前需要发送整个密度更新或者参数向量的方法。<br>详细的可以参考：<br><a href="http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf" target="_blank" rel="noopener">Strom, 2015 - Scalable Distributed DNN Training using Commodity GPU Cloud Computing</a><br><a href="https://blog.skymind.ai/" target="_blank" rel="noopener"> Distributed Deep Learning, Part 1: An Introduction to Distributed Training of Neural Networks.</a><br>相比于之前 Nikko Strom提到的算法这里有很一些新的优点被添加了：  </p>
<ol>
<li>变量阀值：如果更新的数量迭代次数太低，这个阀值会自动的减少通过一个配置的step value。</li>
<li>密度位图编码：如果这个数字更新太高，另外一种编码策略会被使用，将会提供保证“最大数量字节数”传输任何更新信息。  </li>
<li>周期性的，我们发送“抖动”信息，编码一个明显更小的发展，来分享目前阀值无法被共享的延迟权重。  </li>
</ol>
<p><img src="https://deeplearning4j.org/img/distributed.png" alt=""><br>注意使用spark需要开销，为了确定spark是否会帮助你，考虑使用 Performance Listener并且查看迭代毫秒级的时间。如果小于等于150Ms，spark也许不值得使用。  </p>
<h2 id="配置你的集群"><a href="#配置你的集群" class="headerlink" title="配置你的集群"></a>配置你的集群</h2><p>所有你需要做的就是一个spark1.X／2.X的集群，并且最少一个开放的UDP端口（内部绑定和外部绑定都需要）  </p>
<h3 id="集群设置"><a href="#集群设置" class="headerlink" title="集群设置"></a>集群设置</h3><p>如同上面提到的，DeepLearning4j支持Spark1.x和Spark2.x集群。同样也需要java 8+来运行。</p>
<h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>Master和slave节点之间的通讯分享Gradient重度依赖UDP协议。如果是运行在AWS或者Azure上，你需要允许一个UDP端口用来作为内部外部的连接，然后你需要指定对应的端口在VoidConfiguration.unicastPort(int)从而传递到SharedTrainingMaster构造器中。<br>另外一个需要记住的是：如果你在使用YARN，你需要指定网络掩码用来作为UDP通讯。这个可以用来完成通过使用像：VoidConfiguration.setNetworkMask(“10.1.1.0/24”)的语句。<br>一个对于IP地址选择的招数是DL4J_VOID_IP环境变量。在每一个节点设置这个变量，同时使用一个本地地址用于通讯。  </p>
<h3 id="网络掩码"><a href="#网络掩码" class="headerlink" title="网络掩码"></a>网络掩码</h3><p>网络掩码是一个CIDR概念，只是用来告诉软件哪一个网络接口需要用来通讯。比如如果你的集群有三个IP地址：192.168.1.23, 192.168.1.78, 192.168.2.133，它们相同的部分是192.168.*，所以网络掩码是：192.168.0.0/16。你可以从：<a href="https://en.wikipedia.org/wiki/Subnetwork" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Subnetwork</a>获得详细信息。<br>我们在spark运行在Hadoop上时使用或者其他没有假设Spark IP地址被通知的环境。在这个情况下，合法的网络掩码应该在VoidConfiguration中提供，同时被用来选择接口作为Spark外面的通讯。  </p>
<h3 id="Effective-Scalability"><a href="#Effective-Scalability" class="headerlink" title="Effective Scalability"></a>Effective Scalability</h3><p>The longer the original iteration time, the less relative impact will come from sharing, and the better hypothetical scalability you will get.</p>
<p>原始迭代时间越长，共享产生的相对影响就越小，并且您将获得更好的假设可伸缩性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/04/20/Distributed Training- Gradients Sharing/" data-id="cjotm8kl400032xrrmwzhhivp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Deep Learning on Apache Spark" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/15/Deep Learning on Apache Spark/" class="article-date">
  <time datetime="2018-04-15T04:41:12.000Z" itemprop="datePublished">2018-04-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/15/Deep Learning on Apache Spark/">Deep Learning on Apache Spark 翻译</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Deep-Learning-on-Apache-Spark"><a href="#Deep-Learning-on-Apache-Spark" class="headerlink" title="Deep Learning on Apache Spark"></a>Deep Learning on Apache Spark</h1><p>翻译自：<a href="https://deeplearning4j.org/spark#locality" target="_blank" rel="noopener">https://deeplearning4j.org/spark#locality</a><br>Deep learning 是计算密集的，所以有非常大的数据，计算速度。你可以处理这个问题使用很快的硬件，通常是GPU,优化代码和一些形式的并行。<br>数据并行性使大型数据集称为碎片并把这些碎片交给神经网络分离，比如每个神经网络都在自己的核心上。Deeplearning4j依靠spark来处理这些，并发训练模型并且迭代平均它们在中心模型所产生的参数。（模型并发，在<a href="https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf" target="_blank" rel="noopener">here</a>被讨论了，允许使用模型在不同的大数据集上使用而无需被平均）<br>注意如果你需要一个参数服务器基础的方法（需要更多的步骤），可以查看<a href="https://deeplearning4j.org/distributed" target="_blank" rel="noopener">分发页面</a>.</p>
<h2 id="Overview-概述"><a href="#Overview-概述" class="headerlink" title="Overview 概述"></a>Overview 概述</h2><p>Deeplearning4j支持在Spark集群上训练神经网络，为了加速网络训练。<br>和DL4J的MultiLayerNetwork和ComputationGraph类相同，DL4J定义了两个类用于在Spark上训练神经网络。  </p>
<ol>
<li>SparkDl4jMultiLayer 一个围绕MultiLayerNetwork的包装类</li>
<li>SparkComputationGraph 一个围绕ComputationGraph的包装类</li>
</ol>
<p>由于这两个类时围绕标准的单机类包装的，这个网络配置过程对于标准的和分布式的训练都是相同的。在Spark上分布式训练和本地训练有两个不同的地方，1.数据是如何加载的，和训练是如何启动的（需要一些额外的集群配置）。<br>一个标准的在Spark集群上训练网络的工作流程如下：  </p>
<ol>
<li><p>创造你自己的神经网络训练类。通常来说，包含为以下工作所做的代码：<br> 指定你自己的神经网络配置（MultiLayerConfiguration或者ComputationGraphConfiguration），和你为单机训练所配置的那样。<br> 创造一个TrainingMaster的实例：这个指定了分布式训练会如何在实际中进行。<br> 加载你自己的训练数据。这里有很多方法来加载数据，有不同的权衡；更多的细节将会在之后的文档中讨论。<br> 调用合适的fit方法在SparkDl4jMultiLayer或者SparkComputationGraph实例中。<br> 保存或者使用训练的网络（这个训练的MultiLayerNetwork或者ComputationGraph实例）  </p>
</li>
<li><p>将你的jar打包等候spark提交。<br> 如果使用maven，运行“mvn package -DskipTests”是一个方法</p>
</li>
<li><p>调用Spark提交，并且启动你集群使用合适的配置。</p>
</li>
</ol>
<p>注意对一个单独机器的训练，Spark locak可以在DL4J上使用，尽管这不推荐（因为同步化和序列化在spark上的过度使用）。替代的，可以考虑使用以下：  </p>
<ol>
<li>对于一个单机CPU/GPU系统，使用标准的MultiLayerNetwork或者ComputationGraph训练。  </li>
<li>对于多个CPU／GPU系统，使用ParallelWrapper. 这个功能上等于运行spark在本地模式，尽管有更低的开销（从而有更好的训练表现）。</li>
</ol>
<h2 id="How-Distributed-Network-Training-Occurs-with-DL4J-on-Spark"><a href="#How-Distributed-Network-Training-Occurs-with-DL4J-on-Spark" class="headerlink" title="How Distributed Network Training Occurs with DL4J on Spark"></a>How Distributed Network Training Occurs with DL4J on Spark</h2><p>分布式神经网络是如何在DL4J上训练的<br>当前的DL4J版本使用的参数平均的过程在训练一个网络的时候。未来的版本可能会加入其他分布式网络训练方法。<br>The process of training a network using parameter averaging is conceptually quite simple:<br>训练一个网络使用参数平均的过程在概念上很简单： </p>
<ol>
<li>Master（spark driven）开始一个初始化的网络配置和参数</li>
<li>数据被切分为很多子集，基于TrainingMaster的配置</li>
<li>在这些数据切片上迭代。对每一个训练数据的切片：<br> 将参数，配置（如果适当的，对于momentum/rmsprop/adagrad的网络更新状态）从master给到每一个worker中。<br> 在每一个切片的一部分适配每一个worker<br> 平均参数（如果可能，更新状态）并且返回评价结果到mater中。</li>
<li>训练结束，master有一个已经训练好的模型网络。  </li>
</ol>
<p>比如，下图展示了5个worker参数平均的过程一个每一个参数均衡频率是1.就像一个离线的训练，一个训练数据集被切割为一系列的数据子集（通常被认为是minibatch，在非分布式设置中）；在每一个split中训练过程，每一个worker获得这个切片的一个子集。在实际中，这个切片的个数是被自动决定的，基于训练配置（基于worker的数量，平均的频率和worker minibatch Size）。<br><a href="https://deeplearning4j.org/img/parameter_averaging.png" target="_blank" rel="noopener"></a></p>
<h2 id="A-Minimal-Example"><a href="#A-Minimal-Example" class="headerlink" title="A Minimal Example"></a>A Minimal Example</h2><p>本节展示了你需要在spark上训练的要素的最小部分。关于加载数据的各种方法随后就来。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaSparkContext sc = ...;</span><br><span class="line">JavaRDD&lt;DataSet&gt; trainingData = ...;</span><br><span class="line">MultiLayerConfiguration networkConfig = ...;</span><br><span class="line"></span><br><span class="line">//Create the TrainingMaster instance</span><br><span class="line">int examplesPerDataSetObject = 1;</span><br><span class="line">TrainingMaster trainingMaster = new ParameterAveragingTrainingMaster.Builder(examplesPerDataSetObject)</span><br><span class="line">        .(other configuration options)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line">//Create the SparkDl4jMultiLayer instance</span><br><span class="line">SparkDl4jMultiLayer sparkNetwork = new SparkDl4jMultiLayer(sc, networkConfig, trainingMaster);</span><br><span class="line"></span><br><span class="line">//Fit the network using the training data:</span><br><span class="line">sparkNetwork.fit(trainingData);</span><br></pre></td></tr></table></figure></p>
<p>##Using the output from SparkDl4jMultiLayer/ComputationGraph<br>由于spark网络作为一个wrapper围绕multi layer 网络和computation graph apis，你需要最终需要从spark神经网络中获取network在训练结束之后。其原因是由于数据平行训练实际上是在训练期间一次对多个网络进行平均。这意味着这里没有一个网络直到你获得了最后的平均参数在多个worker上积累的集合的输出。<br>了解了这些，我们应该获取底层的引用不论在SparkComputationGraph和SparkDl4jMultiLayer上都使用getNetwork方法。<br>你将会注意到合适的输出将会直接返回相同的底层的网络。在这种情况下， 你可以直接使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaSparkContext sc = ...;</span><br><span class="line">JavaRDD&lt;DataSet&gt; trainingData = ...;</span><br><span class="line">MultiLayerConfiguration networkConfig = ...;</span><br><span class="line"></span><br><span class="line">//Create the TrainingMaster instance</span><br><span class="line">int examplesPerDataSetObject = 1;</span><br><span class="line">TrainingMaster trainingMaster = new ParameterAveragingTrainingMaster.Builder(examplesPerDataSetObject)</span><br><span class="line">        .(other configuration options)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line">//Create the SparkDl4jMultiLayer instance</span><br><span class="line">SparkDl4jMultiLayer sparkNetwork = new SparkDl4jMultiLayer(sc, networkConfig, trainingMaster);</span><br><span class="line"></span><br><span class="line">//Fit the network using the training data:</span><br><span class="line">MultiLayerNetwork outputNetwork = sparkNetwork.fit(trainingData);</span><br></pre></td></tr></table></figure></p>
<h2 id="Configuring-the-TrainingMaster"><a href="#Configuring-the-TrainingMaster" class="headerlink" title="Configuring the TrainingMaster"></a>Configuring the TrainingMaster</h2><p>一个在DL4J中的TrainingMaster是一个接口允许多个不同的训练实现在SparkDl4jMultiLayer和SparkComputationGraph使用。<br>目前DL4J有一个实现，ParameterAveragingTrainingMaster。这个参数平均的过程在上面图片中展示了。为了创造一个，使用以下的构建模式：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TrainingMaster tm = new ParameterAveragingTrainingMaster.Builder(int dataSetObjectSize)</span><br><span class="line">            ... (your configuration here)</span><br><span class="line">            .build();</span><br></pre></td></tr></table></figure>
<p>这个ParameterAveragingTrainingMaster定义了很多的参数选项用于控制训练的执行：  </p>
<p><em>dataSetObjectSize：</em>必须值。这个在build构造函数中知名。这个值指明了有多少的列子在每一个数据集对象中。作为一个通用的规则：<br>    如果你在训练一个预处理过的数据集对象，这个将会是这个预处理的数据集的大小<br>    如果你是直接从string中进行训练，比如CSV数据经过一定的步骤后到一个RDD<dataset>那么这个将会是1.<br><em>batchSizePerWorker：</em>这个控制了每一个worker的minibatch 大小。这个和淡季训练的时候使用minibatch size有些类型。换一种方式来说：这个是对每一个参数进行更新所需要的例子数量在每一个worker中。<br><em>averagingFrequency：</em>这个控制了参数平均和重新分发的频率，根据大小为batchSizePerWorker的minibatches数量计算，通常：<br>    低平均周期（比如，averagingFrequency = 1）也许不太有效率（太多的网络通信和初始化开销，相对于计算来说。）<br>    大平均周期（比如，averagingFrequency = 200）可能会导致低性能（不同的worker实例中的参数也许会有很大的差异）<br>    Averaging periods 在5-10个minibatches也许是一个安全的选择。</dataset></p>
<p><em>workerPrefetchNumBatches：</em>Spark worker可以异步获取一定数目的minibatches，为了避免等待数据的加载。<br>    设定这个值为0来关掉提前获取。<br>    值设定为2通常是一个不错的默认值。太大的值在很多情况下并没有帮助（但是会消耗更多的内存）。<br><em>rddTrainingApproach：</em>从版本0.60开始，DL4J提供两种方法来训练从RDD<dataset>或者RDD<multidataset>.分别是：RDDTrainingApproach.Export和RDDTrainingApproach.Direct方法。<br>        Export:（Default）这个首先保存RDD<dataset>到磁盘中，使用批量和序列化的形式。这个executor然后异步加载DataSet对象，按照需求。这个方法对于大数据集和多个epochs来说比Direct方法表现要好一些。它避免了Direct方法里面切分和重新分区的开销，同样只需要更少的内存。临时文件会被删除使用TrainingMaster.deleteTempFiles()。<br>        Direct：这是DL4J早期使用的版本。它提供了更好的性能对于小数据集来说因为将会全部放进内存中。<br><em>exportDirectory</em>：只被 Export training approach 使用。这个控制了临时文件应该被保存在哪。默认是使用：{hadoop.tmp.dir}/dl4j/目录，而{hadoop.tmp.dir}是Hadoop临时目录性质的值。<br><em>storageLevelStreams</em>：只有在使用fitPaths(RDD<string>)方法的时候使用。这是DL4J将会用来保存RDD<string>的存储级别。<br>默认：StorageLevel.MEMORY_ONLY.这个默认值目前几乎在所有情况下都可以。<br><em>repartition：</em>配置何时数据应该被重新分区。ParameterAveragingTrainingMaster将会进行一个mapParititons的操作；相应的，分区的数量（以及每一个分区的值）和分区的利用有很大的关系。然而，重新分区不是一个自由的操作，有些数据必须通过网络进行复制，以下的操作是可以的：<br>    Always：默认操作，重新分区数据来保证分区的正确数量。推荐的，特别使用RDDTrainingApproach.Export（默认是0.6）或者fitPaths(RDD<string>)。<br>    Never：从不重新分区数据，无论这个分区有多不平衡。<br>    NumPartitionsWorkersDiffers：只有当分区的数量和worker的数量不一致的时候才分区。注意到即使分区的数量和总共内核的数量相同，这也不保证正确的数据集对象的就在每一个分区：有的分区也许有多的数据有的是少的数据。<br><em>repartitionStrategy:</em>哪一个重新分配的策略应该被完成。<br>    Balanced：（默认）这个是DL4J默认的重新分区的策略。它试图确保每一个分区都是均衡的对比SparkDefault选项来说。然而，在实际中，这个需要额外的count 操作来执行；在有些情况（主要是在小型的网络里，或者那些小数量的计算在每一个minibatch），这个好处也许比不上额外的执行开销。推荐的，特别在使用RDDTrainingApproach.Export 或者fitPaths（RDD<string>）<br>    SparkDefault:    这个是Spark的标准分区策略。本质上，每一个在初始化RDD中的对象被随机映射到N个中RDD中的一个。因此，这个分区也许不是最好的平衡，在小型的RDD的时候特别会有问题，特别是当它们使用预处理数据集对象并且频繁均衡时期（简单因为随机采样变化）。</string></string></string></string></dataset></multidataset></dataset></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/04/15/Deep Learning on Apache Spark/" data-id="cjotm8kkw00002xrrndcsl5dc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning4j-spark/">deeplearning4j; spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-RDDSpark" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/04/RDDSpark/" class="article-date">
  <time datetime="2018-04-04T06:31:10.000Z" itemprop="datePublished">2018-04-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/04/RDDSpark/">RDD Programming Guide 翻译</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="RDD-Programming-Guide"><a href="#RDD-Programming-Guide" class="headerlink" title="RDD Programming Guide"></a>RDD Programming Guide</h1><p>from <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在一个较高的层级，每一个Spark 应用包含了一个driven程序运行用户的main函数以及运行不同的并行操作在一个集群上。<br>Spark主要提供的抽象是一个弹性分布试数据集（RDD），是一个元素集分布在不同的集群节点可以被并行操作的。RDDs在Hadoop文件系统中由一个文件开始创造的。或者一个在驱动程序中存在的Scala集合并且改变它。用户也许会请求Spark来永久存储一个RDD在内存中，然后可以有效的在并行操作中再次访问。最终，RDDs自动从节点失败中恢复。<br>Spark提供的第二个抽象是共享变量，可以在并行操作中使用。默认的，当Spark并行运行一个函数作为一系列的任务在不同的节点中，它将每一个函数中的变量传输到每一个节点然后给到每一个任务。有时，一个变量需要在不同的任务直接共享或者需要在任务和driven 程序直接共享。Spark提供两种类型的共享变量：<b>broadcast</b> 变量，可以用来缓存值在所有的节点中。 <b> accumulators</b>,变量只是用于“added”，比如counter和sums。  </p>
<h2 id="Initializing-Spark（Java）"><a href="#Initializing-Spark（Java）" class="headerlink" title="Initializing Spark（Java）"></a>Initializing Spark（Java）</h2><p>spark程序首先需要创建一个JavaSparkContext对象，告诉Spark如何访问一个集群。为了创建一个SparkContext你首先需要创建一个SparkConf对象，包含你应用的信息。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br></pre></td></tr></table></figure>
<p>appName 参数是你的应用名字将会展示在集群UI上。 master可能是一个spark，Mesos或者YARN集群URL，或者一个特殊的“local“ 来本地运行。在实际使用中，当在集群中运行时，你不会想要hardcode master在程序中，而是启动应用使用 spark-submit 并且接受它。在本地测试和单元测试的时候，你可以传递”local“并且运行spark  </p>
<h2 id="Resilient-Distributed-Datasets（RDDs）弹性分布式数据集"><a href="#Resilient-Distributed-Datasets（RDDs）弹性分布式数据集" class="headerlink" title="Resilient Distributed Datasets（RDDs）弹性分布式数据集"></a>Resilient Distributed Datasets（RDDs）弹性分布式数据集</h2><p>Spark是以RDD观念为中心的，是一个容错的元素集可以并行进行操作。这里有两种方式来创造RDDs：在driver程序中并行化一个已存在的集合，或者引用一个数据集在一个外部存储系统，比如一个文件系统，HDFS，HBase或者任何其他的提供一个Hadoop输入格式的数据源。  </p>
<h3 id="并行化集合"><a href="#并行化集合" class="headerlink" title="并行化集合"></a>并行化集合</h3><p>并行化集合是调用JavaSparkContext的parallelize方法来创造的。这个元素的集合会被复制称为一个分布式数据集，然后可以用来并行化操作。比如，这是如何创造一个并行化集合来存储数字1到5:  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);</span><br><span class="line">JavaRDD&lt;Integer&gt; distData = sc.parallelize(data);</span><br></pre></td></tr></table></figure>
<p>一旦创造了，这个分布式数据集（distData）可以被用来并行操作。比如，我们也许会调用distData.reduce((a,b) -&gt; a + b) 来计算元素的列表的和。<br>一个重要的参数来并行化几个是将这个数据集拆分为几个partitions的个数。Spark将会在每一个集群的partition运行一个任务。一般的你想要一个CPU有2-4个partition在你的集群中。通常，Spark尝试自动设置partitions基于你的集群。同时你也可以手动的设置通过传递一个秒参数到parallelize（比如：sc.parallelize(data,10)）.</p>
<h2 id="RDD-操作"><a href="#RDD-操作" class="headerlink" title="RDD 操作"></a>RDD 操作</h2><p>RDDs支持两种类型的操作：变化，从已知的数据集中创造一个新的数据集，并且actions（执行），将会返回一个值给driver program在dataset运行了计算以后。比如，map是一个变化，将数据集中的每一个元素都运行一遍函数，然后返回新的RDD来代表结果。在另一方面，reduce也是一个action聚合所有的RDD元素使用某个函数然后返回最终的结果到driver program（尽管这仍然有一个并行的 reduceByKey返回一个分布式数据集）。<br>Spark中所有的变化都是lazy的，它们不会立刻的计算结果。反而，它们只是记住了有些基本的数据集需要执行一些transformations。这些transformations只会在最后需要一个结果返回给driver program时才会执行。这个设计保证了Spark可以有效率的运行。比如，我们可以认识到一个创造了map的数据集将会在reduce中使用，并且只返回reduce的结果到driver中，而不是一个庞大的mapped数据集。<br>默认的，这样的变化的RDD每次你运行一个action在上面的时候都会需要重新计算。你也可以持久化一个RDD在内存中使用persist或者cache方法，Spark将会保存元素在集群中以便快速访问。它也支持持久化RDDs到硬盘中，或者在多个节点之间进行复制。  </p>
<h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><p>展示RDD的基础，考虑以下这个简单的程序：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());</span><br><span class="line">int totalLength = lineLengths.reduce((a, b) -&gt; a + b);</span><br></pre></td></tr></table></figure>
<p>第一行定义了一个基础的RDD从外部文件中获取的。这个数据集没有加载到内存中或者采取了行动：lines只是一个指向文件的指针。第二行定义了lineLengths作为一个map变化的结果。同样lineLengths也不能立即的计算，应为laziness。最终，我们运行reduce，是一个action。在这个点，spark将计算分为多个任务并且在不同的独立机器中运行，每一个运行自己部分的map和一个本地的reduction，然后这个结果到driver program。<br>如果我们仍然想要使用lineLengths，我们可以增加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lineLengths.persist(StorageLevel.MEMORY_ONLY());</span><br></pre></td></tr></table></figure>
<h3 id="传递参数到Spark"><a href="#传递参数到Spark" class="headerlink" title="传递参数到Spark"></a>传递参数到Spark</h3><p>Spark的API严重依赖在driver program中传递函数来运行集群。在Java，functions被实现了org.apache.spark.api.java.function package中的接口的class所代表。这里有两种方式来创造函数：  </p>
<ol>
<li>在你自己的类中实现函数接口，不管是匿名类或者有名字的，并且传递一个实例到spark。</li>
<li>使用lambda expression来简洁的定义一个实现。  </li>
</ol>
<p>比如我们可以定义函数如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(new Function&lt;String, Integer&gt;() &#123;</span><br><span class="line">  public Integer call(String s) &#123; return s.length(); &#125;</span><br><span class="line">&#125;);</span><br><span class="line">int totalLength = lineLengths.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">  public Integer call(Integer a, Integer b) &#123; return a + b; &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>或者写函数是笨重的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class GetLength implements Function&lt;String, Integer&gt; &#123;</span><br><span class="line">  public Integer call(String s) &#123; return s.length(); &#125;</span><br><span class="line">&#125;</span><br><span class="line">class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123;</span><br><span class="line">  public Integer call(Integer a, Integer b) &#123; return a + b; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());</span><br><span class="line">int totalLength = lineLengths.reduce(new Sum());</span><br></pre></td></tr></table></figure></p>
<p>注意到java的匿名函数同样也可以访问变量在enclosing范围内，只要它们被标记为final。Spark将会传输这些变量到工作节点就像它会给其他的语言所做的那样。  </p>
<h3 id="了解closures"><a href="#了解closures" class="headerlink" title="了解closures"></a>了解closures</h3><p>Spark较为困难的一件事情是理解范围和变量和方法的生命周期当代码在集群中运行的时候。RDD操作改变了变量的外部范围常常容易让人困惑。在下面的例子中，我们会看到使用foreach()来增加一个counter，但是同样的问题也会在其他的操作中发生。  </p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>考虑原生的RDD元素和，将会根据是否在相同的JVM上运行会有不同的结果。一个普通的例子是执行spark在local模式（–master = local[n]）对比部署一个spark到一个集群中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int counter = 0;</span><br><span class="line">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(data);</span><br><span class="line"></span><br><span class="line">// Wrong: Don&apos;t do this!!</span><br><span class="line">rdd.foreach(x -&gt; counter += x);</span><br><span class="line"></span><br><span class="line">println(&quot;Counter value: &quot; + counter);</span><br></pre></td></tr></table></figure>
<h4 id="Local-VS-cluster-modes"><a href="#Local-VS-cluster-modes" class="headerlink" title="Local VS. cluster modes"></a>Local VS. cluster modes</h4><p>上面的代码的行为是未定义的，而且可能运行并不像所预期的。为了执行job，spark将RDD操作切成多个任务，每一个在一个executor中执行。在执行之前，Spark会计算这个任务的closure。Closure是哪些变量和方法必须对这个executor可见来执行对RDD的计算（在这个例子中foreach()）。这个closure是序列化的并且被发给每一个executor。<br>被发给每一个executor的这个closure的变量会被复制并且然后当counter在这个foreach函数中被引用时，它不再是在driver节点上的counter。这里仍然有一个counter在driver节点的内存中，但是不在对executor可见了。executors只能看到序列化的closure的副本。这样最终的counter仍然将会是0因为所有在counter操作都是对序列化closure上的值的操作。<br>在本地模式，有时，foreach函数将会实际在和drive上的同一个jvm中执行，同时会引用原来的counter，这个时候就会更新它。<br>为了保证这种情况的可控性，应该使用一个 Accumulator。Accumulators在spark中通常用来提供一个机制来安全的更新一个变量，当执行被切分到不同的worker节点中时。这个Accumulators在后续还会被讨论。<br>通常说来， closures - constructs喜欢循环活着本地定义方法，不应该被用来改变一些全局的状态。Spark不能定义或者保证对对象的改变。一些代码也许会在本地模式其效果，但是也只是偶然，同时在分布式模式也不能和期望一样运行。使用一个Accumulators来取代一些全局变量的聚合是必要的。  </p>
<h4 id="打印一个RDD的元素"><a href="#打印一个RDD的元素" class="headerlink" title="打印一个RDD的元素"></a>打印一个RDD的元素</h4><p>另外一个常见的行为是试图使用rdd.foreach(println)或rdd.map(println)打印一个RDD元素。在单机情况下，这个将会输出期望并且打出所有的RDD元素。然而在集群的情况下，这个stdout的输出只会被executor所调用并且写到executor的stdout中，而不是在driver node上，所以driver上并不会输出这些内容。为了打出所有在driver上的元素，一个可以使用collect()方法来将RDD放到driver上：rdd.collect().foreach(println)。这个会引起driver内存溢出，因为collect()将全部的RDD放到一台机器上，如果你只需要打出RDD中的部分元素，一个安全的方法是使用take():<br>rdd.take(100).foreach(println).  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/04/04/RDDSpark/" data-id="cjotm8kl600042xrr14emfy08" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-sparkLearningNote" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/02/sparkLearningNote/" class="article-date">
  <time datetime="2018-04-02T13:57:35.000Z" itemprop="datePublished">2018-04-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/02/sparkLearningNote/">Sparks 的一些名词</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Some-terms-with-Spark"><a href="#Some-terms-with-Spark" class="headerlink" title="Some terms with Spark"></a>Some terms with Spark</h1><p><a href="https://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/cluster-overview.html</a></p>
<h3 id="excutor"><a href="#excutor" class="headerlink" title="excutor"></a>excutor</h3><p>A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.<br>在一个工作节点运行的一个进程，用来运行任务，保存数据在内存或者硬盘中。每一个应用有自己的executors。</p>
<h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><p>User program built on Spark. Consists of a driver program and executors on the cluster.<br>在Spark上的用户程序。有一个驱动程序和executor在集群中组成。  </p>
<h3 id="Application-jar"><a href="#Application-jar" class="headerlink" title="Application jar"></a>Application jar</h3><p>A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.  </p>
<p><b>一个包含了用户spark应用的程序。在有的地方用户将会创造一个“混合jar”将她们的应用和依赖一起包含。这个用户的jar不应该包含hadoop或者spark的包，它们将会在运行时被添加。 ？？？ 这段没看懂</b></p>
<h3 id="Driver-program"><a href="#Driver-program" class="headerlink" title="Driver program"></a>Driver program</h3><p>The process running the main() function of the application and creating the SparkContext<br>创造应用的main()函数并且创造SparkContext</p>
<h3 id="Cluster-manager"><a href="#Cluster-manager" class="headerlink" title="Cluster manager"></a>Cluster manager</h3><p>An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)<br>一个外部的服务用于获取集群的资源。（比如standalone manager，Mesos， YARN）</p>
<h3 id="Deploy-mode"><a href="#Deploy-mode" class="headerlink" title="Deploy mode"></a>Deploy mode</h3><p>Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.<br>区分驱动进程在哪运行。在“cluster”模式下，这个框架启动驱动在集群内部。在“client”模式下，submitter在集群外启动进程。  </p>
<h3 id="Worker-node"><a href="#Worker-node" class="headerlink" title="Worker node"></a>Worker node</h3><p>Any node that can run application code in the cluster<br>在集群中任意节点可以运行应用代码。</p>
<h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><p>A unit of work that will be sent to one executor<br>一组会被发送到一个executor的工作。</p>
<h3 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h3><p>A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.<br>一个包含了多个任务的并行计算用来响应一个Spark action。</p>
<h3 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h3><p>Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.<br>每一个工作被分为很小的任务集被称为stages；你可以在driver的日志中看到它们。</p>
<h2 id="组成部分"><a href="#组成部分" class="headerlink" title="组成部分"></a>组成部分</h2><p>Spark application是作为一个独立的进程集合在集群上运行，通过你的主程序（称为driver program）中的SparkContext来协调工作。<br>特别的，在集群上运行，SparkContext可以链接不同的集群管理（either Spark’s own standalone cluster manager, Mesos or YARN），用来在应用中分配资源。一旦连接，Spark获取集群中节点上的executors，是你应用中的进程用于计算和存储数据。下一步它会发送你的应用程序（通过JAR或者Python文件定义的传递给SparkContext）到executors。最终，SparkContext发送任务到exexutor来运行。<br>在这个架构中有以下一些事情需要注意：  </p>
<ol>
<li>每一个应用获取它自己的executor进程，那些进程会在整个应用中启动，并且运行任务在多个线程里。<br> 这有利于对每一个应用进行隔离，在调度方面（每一个驱动调度自己的任务）和执行方面（不同的应用在不同的JVM中运行）。这也意味着数据不能在不同的Spart 应用中共享（SparkContext的实例）除非将它们写到一个另外的存储系统中。</li>
<li>Spark对于底层的集群管理是不可知的。只要它可以获取到处理器的进程，以及它们之间的通讯，它可以很容易的在支持其他的应用的集群管理器上运行。  </li>
<li>Driver程序在它的生命周期必须监听并且从它的executor中接受会来的连接。所以driven程序必须网络可达对于worker节点。  </li>
<li>由于driver 在集群中调度task，它应该在接近worker节点的地方运行，偏向于在同一个网络。如果你想要远程发送请求，那么最好对driver打开RPC并且从附近提交操作而不是运行一个很遥远的driver。  </li>
</ol>
<p><b>When running Spark on YARN, each Spark executor runs as a YARN container.<br>Where MapReduce schedules a container and starts a JVM for each task, Spark hosts multiple tasks within the same container.</b><br>所以executor就是对应的container，然后原来MapReduce是一个Container运行一个任务，Spark可以在一个Container中运行多任务。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/04/02/sparkLearningNote/" data-id="cjotm8klh000b2xrrpq4y0bch" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-LSTM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/02/LSTM/" class="article-date">
  <time datetime="2018-04-02T06:05:05.000Z" itemprop="datePublished">2018-04-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/02/LSTM/">LSTM Walk Through 翻译</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="LSTM-Walk-Through"><a href="#LSTM-Walk-Through" class="headerlink" title="LSTM Walk Through"></a>LSTM Walk Through</h1><p>reference: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>在下面这个LSTM模型中，需要决定的第一点就是我们将要丢掉什么从cell 状态中。<br>这个决定是被一个称为：“forget gate layer” 的sigmoid 层来决定的。它将 \( h_{t-1}\) 和 \(x_t\)作为输入，并且输出一个介于0到1之间的值在cell state \(C_{t-1}\). 1代表完全的保存而0代表完全丢掉。<br>接下来我们看回之前的语言模型，试图根据前一个值来预测下一个值。在这个问题中，这个Cell state可能包括包括当前客体的性别，所以正确的代词才会被使用。当我们看到一个新的客体时，我们想要忘掉老的客体。<br><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt=""><br>下一步是决定哪些新的信息我们想要存储到cell state中。这里有两个部分：  </p>
<pre><code>1. 一个sigmoid层，称为“input gate layer”，决定哪些值，我们将会更新。
2. 一个tanh层，创造一个新的候选值的向量，\\( C_t^~ \\),将会用来加入到状态中。在下一步，我们将会组合这两个然后来在状态里创造一个升级。
</code></pre><p>在我们的语言模型中，我们想要增加新客体的性别到cell state中，来替换我们忘掉的老的。</p>
<p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt=""><br>现在是时候升级老的cell state，\(C_{t-1} \)到新的cell state \( C_t\).前一个步骤已经实际决定了怎么做，我们只需要真正实施就好。<br>我们对老的state乘 \( f_t\),来忘记我们之前决定忘记的。然后我们加上 \( i_t * C_t^{~} \). 这是新的候选值，通过我们决定升级多少的状态来扩展。<br>在这个语言模型中，这是我们实际丢掉老客体的性别和添加它们新的信息的地方。<br><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt=""></p>
<p>最终我们需要决定我们想要输出什么。这个输出将会基于我们的cell state，但是将会是一个filtered 版本。首先我们运行一个sigmoid 层级，来决定cell state哪一部分我们将会输出，然后我们将这个cell state进行tanh处理（将这个值输出为-1 到 1 之间）然后和sigmoid gate的输出相乘，这样我们只输出我们想要的部分。  </p>
<p>对于语言模型例子，由于它只是看到一个客体，也许想要输出信息和动词相关。比如它可能输出是否这个客体是单数或者复数，这样我们知道接下来应该使用什么形式的动词。<br><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt=""></p>
<h2 id="Variances-in-Code"><a href="#Variances-in-Code" class="headerlink" title="Variances in Code"></a>Variances in Code</h2><p><b>batch_size:</b>denotes the subset size of your training sample (e.g. 100 out of 1000) which is going to be used in order to train the network during its learning process. Each batch trains network in a successive order, taking into account the updated weights coming from the appliance of the previous batch.<br>在学习过程中将会用来训练你的模型的数量。每一个batch训练network在一个连续的顺序，会不断的更新权重。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.sevenpan.com/2018/04/02/LSTM/" data-id="cjotm8kkz00012xrrd0s55pif" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algotrading-pyalgotrade/">Algotrading, pyalgotrade</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deeplearning4j-spark/">deeplearning4j; spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo-git-nginx/">hexo,git,nginx</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algotrading-pyalgotrade/" style="font-size: 10px;">Algotrading, pyalgotrade</a> <a href="/tags/deeplearning4j-spark/" style="font-size: 10px;">deeplearning4j; spark</a> <a href="/tags/hexo-git-nginx/" style="font-size: 10px;">hexo,git,nginx</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/23/RPC,REST,SOAP/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/11/07/配对交易策略RQ/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/10/17/Somethings About Pyalgotrade/">Somethings About Pyalgotrade  - the Optimizing</a>
          </li>
        
          <li>
            <a href="/2018/10/13/hexoWithGit/">Hexo With Git</a>
          </li>
        
          <li>
            <a href="/2018/04/23/Shuffle_翻译自spark_programming_guideline/">Spark Programming--- Shuffle Operations 翻译</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Pan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>